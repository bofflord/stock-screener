{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd07af08b5645c98da37631965eb8e1dc7c1027747cec1e8350e3d8d426b79d2a71",
   "display_name": "Python 3.8.1 32-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "7af08b5645c98da37631965eb8e1dc7c1027747cec1e8350e3d8d426b79d2a71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting findspark\n",
      "  Using cached findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n",
      "WARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\boffmasta\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+\n|hello|\n+-----+\n|spark|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- Date: date (nullable = true)\n |-- Ticker: string (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adj Close: double (nullable = true)\n |-- Volume: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, DateType\n",
    "\n",
    "def load_ticker_prices(spark, symbol_list):\n",
    "    # WORK IN PROGRESS\n",
    "    #file_list = glob.glob('../data/3_prices/*.csv')\n",
    "    file_list = glob.glob('../data/3_prices/*.csv')\n",
    "    file_list = [file_path for file_path in file_list\\\n",
    "                    if any(symbol in file_path for symbol in symbol_list)]\n",
    "    schema = StructType() \\\n",
    "                .add(\"Date\",DateType(),True) \\\n",
    "                .add(\"Ticker\",StringType(),True) \\\n",
    "                .add(\"Open\",DoubleType(),True) \\\n",
    "                .add(\"High\",DoubleType(),True) \\\n",
    "                .add(\"Low\",DoubleType(),True) \\\n",
    "                .add(\"Close\",DoubleType(),True) \\\n",
    "                .add(\"Adj Close\",DoubleType(),True) \\\n",
    "                .add(\"Volume\",DoubleType(),True) \n",
    "    df = spark.read.option('header',True)\\\n",
    "                    .schema(schema)\\\n",
    "                    .csv(file_list)\n",
    "    df.printSchema()\n",
    "    return df\n",
    "\n",
    "symbol_list =['AAPL']\n",
    "\n",
    "df = load_ticker_prices(spark, symbol_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "datetime.date(2021, 4, 16)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "df.agg({\"Date\": \"max\"}).collect()[0]['max(Date)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# WORK IN PROGRESS\n",
    "# annual price = median of daily low prices in month December\n",
    "def get_annual_historic_price(spark, df, period_dict):\n",
    "\n",
    "    # calculate year and month\n",
    "    df = df.withColumn('year', f.year('Date'))\n",
    "    df = df.withColumn('month', f.month('Date'))\n",
    "    curr_yr = df.agg({\"year\": \"max\"}).collect()[0]['max(year)']\n",
    "    df = df.filter(df[''])\n",
    "    # filter for date > (period_dict['end_date'] - 5)\n",
    "    # df = df.filter\n",
    "    # filter for month December\n",
    "    # tbd\n",
    "    # group by Ticker and calculate median of low value -> df['Low']\n",
    "    \n",
    "\n",
    "    return df_hist\n",
    "\n",
    "\n",
    "\n",
    "period_dict = {'start_date':2010,\n",
    "                'end_date':2019}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}